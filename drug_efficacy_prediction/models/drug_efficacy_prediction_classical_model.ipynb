{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aims to build a model to predict drug efficacy of molecules.\n",
    "\n",
    "A pre-processed HIV dataset with 3 classess (CA - Confirmed active, CM - Confirmed moderately active, CI - Confirmed inactive and benign) is available [here](http://moleculenet.ai/datasets-1). The raw data is available [here](https://wiki.nci.nih.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data)\n",
    "We perform stratified random splitting on the dataset: 80% of the images are in train set and 20% of the images are in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn import config_context\n",
    "import sys\n",
    "from warnings import catch_warnings, simplefilter\n",
    "from logging import warning\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, confusion_matrix\n",
    "import pickle\n",
    "import warnings\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "dirname = os.path.dirname(path)\n",
    "sys.path.append(dirname+'\\\\util')\n",
    "import configs\n",
    "from data import read_data, get_prediction_score\n",
    "#from util import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## FOLDER STURCTURE ######\n",
    "##########################\n",
    "WORK_DIRECTORY = 'C:/Users/liam.bui/Desktop/drug-efficacy/'\n",
    "DATA_FILE = 'HIV.csv'\n",
    "\n",
    "##########################\n",
    "## EVALUATION METRICS ####\n",
    "##########################\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "METRIC_F1_SCORE = 'f1-score'\n",
    "METRIC_COHEN_KAPPA = 'Cohen kappa'\n",
    "METRIC_CONFUSION_MATRIX = 'Confusion Matrix'\n",
    "\n",
    "###############\n",
    "## MODEL ######\n",
    "###############\n",
    "CLASSES = ['benign', 'malignant']\n",
    "TEST_RATIO = 0.2\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path, col_smiles='smiles', col_target='HIV_active'):\n",
    "    \"\"\"Split original data into train data and test data.\n",
    "    :param data_path: str, path to the a CSV data file\n",
    "    :param col_smiles: str, name of smiles column\n",
    "    :param col_target: str, name of target column\n",
    "    :param test_ratio: float, proportion of the original data for testset, must be from 0 to 1\n",
    "    :param seed: int, randomization seed for reproducibility\n",
    "    :return (X, y)\n",
    "    \"\"\"\n",
    "    \n",
    "    # read data\n",
    "    df = pd.read_csv(data_path, sep=',')\n",
    "    df_no_na = df[[col_smiles, col_target]].dropna()\n",
    "\n",
    "    X = df_no_na[col_smiles]\n",
    "    y = df_no_na[col_target].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "                \n",
    "def get_prediction_score(y_label, y_predict):\n",
    "    \"\"\"Evaluate predictions using different evaluation metrics.\n",
    "    :param y_label: list, contains true label\n",
    "    :param y_predict: list, contains predicted label\n",
    "    :return scores: dict, evaluation metrics on the prediction\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    scores[config.METRIC_ACCURACY] = accuracy_score(y_label, y_predict)\n",
    "    scores[config.METRIC_F1_SCORE] = f1_score(y_label, y_predict, labels=None, average='macro', sample_weight=None)\n",
    "    scores[config.METRIC_COHEN_KAPPA] = cohen_kappa_score(y_label, y_predict)\n",
    "    scores[config.METRIC_CONFUSION_MATRIX] = confusion_matrix(y_label, y_predict)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, parameters, n_splits=3):\n",
    "    '''Train model with Grid-Search corss validation to find the best hyparameter\n",
    "    :param model: Scikit-Learn estimator\n",
    "    :param X_train : train set features\n",
    "    :param y_train: trainset label\n",
    "    : param parameters: dict, key is hyper parametr name and value is a list of jyper parameter values\n",
    "    return best_estimator: Scikit-learn estimator with the best hyper parameter\n",
    "    :return best_score: best accuracy score\n",
    "    :return best_score: best accuracy score\n",
    "    :return best_param: dict, best hyper parameter\n",
    "    '''\n",
    "\n",
    "    splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X=X_train, y=y_train)\n",
    "    clf = GridSearchCV(model, parameters, cv = splits, scoring=make_scorer(accuracy_score))\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.catch_warnings('ignore')\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf.best_estimator_, clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_trin, X_test,y_test):\n",
    "    ''''\n",
    "    Evaluae model on testset\n",
    "    :param X_train: trainset features\n",
    "    :param y_train: trainset label\n",
    "    :param X_test: testset features\n",
    "    :param ytest: testset label\n",
    "    :param parameters dict, key is hyper arameter name and value is a list of hyper parameter values\n",
    "    :return model: Scikit-lean estimator, fited on the whole trainset\n",
    "    :return y_predict: prediction on test set\n",
    "    :return scores: dict, evalutation metrics on test set\n",
    "    '''\n",
    "\n",
    "    # Refit the model on the whole train set\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_predict = model.predict(X_test)\n",
    "    scores = None\n",
    "    if y_test is not None:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            scores = accuracy_score(y_test, y_predict)\n",
    "    return model, y_predict, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_models(X_train, y_train):\n",
    "    ''' Train and evaluate different base models. \"Base\" means the model is not a stacking model.\n",
    "    :param X_train: trainset model\n",
    "    :param y_train: trainset label\n",
    "    :return fitted_models: list, contains fitted Scikit-learn estimators\n",
    "    :return model_names: list, names of fitted Scikit-learn estimators\n",
    "    :return model_scores: list, contains scores on test set for fitted Scikit-learn estimators;\n",
    "                        Each score is a dict of evaluation metrics\n",
    "    '''\n",
    "    #######################\n",
    "    # DEFINE BASE MODELS ##\n",
    "    #######################\n",
    "    \n",
    "    models = []\n",
    "    model_params = []\n",
    "    model_names = []\n",
    "\n",
    "    # Random forest model\n",
    "    for n_estimators in [500, 1000, 2000]:\n",
    "        for max_depth in [3, 5, 7]:\n",
    "            models.append(RandomForestClassifier(max_features='sqrt', class_weight='balanced',random_state=0))\n",
    "            model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth]})\n",
    "            model_names.append('Random Forest')\n",
    "    \n",
    "    # Gradient Boost\n",
    "    for n_estimators in [500, 1000, 2000]:\n",
    "        for max_depth in [3, 5, 7]:\n",
    "            for learning_rate in [0.01, 0.1]:\n",
    "                models.append(GradientBoostingClassifier(subsample=0.7, max_features='sqrt', random_state=0))\n",
    "                model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth], 'learning_rate':[learning_rate]})\n",
    "                model_names.append('Gradient Boosting Machine')\n",
    "\n",
    "    # Support Vector Machine\n",
    "    for kernel in ['linear', 'rbf']:\n",
    "        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
    "            models.append(SVC(probability=True, gamma ='auto', tol=0.001, cache_size=200, class_weight='balanced', random_state=0, decision_function_shape='ovr'))\n",
    "            model_params.append({'kernel':[kernel],'C':[C]})\n",
    "            model_names.append('Support Vector Machine')\n",
    "            \n",
    "    # Losgistic regression model\n",
    "    for penalty in ['11', '12']:\n",
    "        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
    "            models.append(linear_model.LogisticRegression(max_iter=500, solver='liblinear', multi_class='ovr',\n",
    "                                                            class_weight='balanced', random_state=0))\n",
    "            model_params.append({'penalty':[penalty], 'C':[C]})\n",
    "            model_names.append('Logistic Regression')\n",
    "    \n",
    "    # K-nearest Neighbor\n",
    "    for n_neighbors in [5, 10, 15]:\n",
    "        for weights in ['uniform', 'distance']:\n",
    "            models.append(KNeighborsClassifier())\n",
    "            model_params.append({'n_neighbors':[n_neighbors], 'weights':[weights]})\n",
    "            model_names.append('K Nearest Neighbor')\n",
    "\n",
    "    ##################################\n",
    "    # TRAIN AND EVALUATE BASE MODELS #\n",
    "    ##################################\n",
    "    fitted_models = []\n",
    "    model_scores = []\n",
    "    for i in range(len(models)):\n",
    "        print(\"Evaluating model {} of {}: {}\".format((i+1), len(models), model_names[i]))\n",
    "        model = models[i]\n",
    "        fitted_cv, _, _ = train_model(model=model, X_train= X_train, y_train=y_train, parameters= model_params[i])\n",
    "        fitted_whole_set, _, score = evaluate_model(model=fitted_cv, X_train= X_train, y_train=y_train, X_test= X_test, y_test=y_test)\n",
    "        fitted_models.append(fitted_models)\n",
    "        model_scores.append(score)\n",
    "        print(model_names[i], score)\n",
    "    \n",
    "    return fitted_models, model_names, model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stack_models(base_models,X_train, y_train):\n",
    "    \"\"\"Train and evaluate different stack models\n",
    "    :param base models: list, contians fitted base models, which are Scikit-learn estimators\n",
    "    :param X_train: trainset features\n",
    "    :param y_train: trainset label\n",
    "    :return stack_fitted_models: list, contains fitted Scikit-learn estimators\n",
    "    :return stack_model_names: list, names of fitted Scikit-leanr estimators\n",
    "    :return stack_model_scores: list, contains scores on test set for fitted Scikit-learn estimators.\n",
    "                                        each score is a dict of evaluation metrics\n",
    "    \n",
    "    \"\"\"\n",
    "    ##############################\n",
    "    ### PREPARE DATA FOR STACING #\n",
    "    ##############################\n",
    "    print('Preparing data for model stacking')\n",
    "    # Get base modles's prediction for test set: simplyuse the tained models to predict on test set\n",
    "    X_test_stack = np.zeros([X_test.shape[0], len[base_models]])\n",
    "    for i in range(len(base_models)):\n",
    "        model = base_models[i]\n",
    "        X_test_stack[:,i] = model.predict(X_test)\n",
    "\n",
    "    # Get base model's prediction for train set: use 3-fold split, train model on 2nd parts and paredict on 3rd part\n",
    "    splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X=X_train,y=y_train)\n",
    "    X_train_staack = np.zeros([X_train.shape[0], len(base_models)])\n",
    "    for train_index, val_index in splits:\n",
    "        # train and validation set\n",
    "        X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "        y_tr, _ = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        #Fit model\n",
    "        for i in range(len(base_models)):\n",
    "            model = base_models[i]\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                model.fit(X_tr, y_tr)\n",
    "            X_train_stack[val_index,i] = model.predict(X_val)\n",
    "\n",
    "    # Add base model's predictions into the feature space\n",
    "    X_train_stack = np.concatenate([X_train, X_train_stack], axis=-1)\n",
    "    X_test_stack = np.concatenate([X_test, X_test_stack], axis=-1)\n",
    "\n",
    "    #########################\n",
    "    # DEFINE STACK MODELS ###\n",
    "    #########################\n",
    "    stack_models = []\n",
    "    stack_model_names = []\n",
    "    stack_model_params = []\n",
    "\n",
    "    # logistic regression\n",
    "    stack_models.append(linear_model.LogisticRegression(max_iter=500, solver='liblinear',multi_class='ovr', class_weight='balanced', random_state=0))\n",
    "    stack_model_names.append('Stack Logistic Regression')\n",
    "    stack_model_params.append({'penalty':['11','12'], 'C':[1.0,10.0, 100.0, 1000.0]})\n",
    "\n",
    "    # Support Vecot Machine\n",
    "    stack_models.append(SVC(probability=True, gamma='auto', tol=0.001, cache_size=200, class_weight= 'balanced', random_state=0, decision_function_shape='ovr'))\n",
    "    stack_model_names.append('Stack Support Vector Machine')\n",
    "    stack_model_params.append({'kernel':['linear','rbf'], 'C':[1.0, 10.0, 100.0, 1000.0]})\n",
    "\n",
    "    # Random Forest\n",
    "    stack_models.append(RandomForestClassifier(class_weight='balanced',random_state=0))\n",
    "    stack_model_names.append('Stack Random Forest')\n",
    "    stack_model_params.append({'n_estimators':[500, 1000, 2000], 'max_depth':[3, 5, 7]})\n",
    "\n",
    "    # Gradient Boosting\n",
    "    stack_models.append(GradientBoostingClassifier(subsample=0.7, max_features='sqrt', learning_rate=0.01, random_state=0))\n",
    "    stack_model_names.append('Stack Gradient Boosting Machine')\n",
    "    stack_model_params.append({'n_estimators':[500, 1000, 2000], 'max_depth':[3, 5, 7],'learning_rate':[0.01, 0.1]})\n",
    "\n",
    "    # K-nearest Nieghbor\n",
    "    stack_models.append(KNeighborsClassifier())\n",
    "    stack_model_names.append('Stack KNN')\n",
    "    stack_model_params.append({'n_neighbors':[5, 10, 15], 'weights':['uniform','distance']})\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    # EVALUATE STACK MODELS #\n",
    "    #########################\n",
    "    stack_fitted_models = []\n",
    "    stack_model_scores = []\n",
    "    for i in range(len(stack_models)):\n",
    "        print(\"Evaluating model {} of {}: {}\".format((i+1), len(stack_models), stack_model_names[i]))\n",
    "        model = stack_models[i]\n",
    "        fitted_cv, _, _ = train_model(model, X_train = X_train_stack, y_train=y_train, parameters=stack_models[i])\n",
    "        fitted_whole_set, _, score = evaluate_model(model=fitted_cv, X_train= X_train_stack, y_train=y_train, X_test=X_test_stack, y_test=y_test)\n",
    "        stack_fitted_models.append(fitted_whole_set)\n",
    "        stack_model_scores.append(score)\n",
    "        print(stack_model_names[i], score)\n",
    "\n",
    "    return stack_fitted_models, stack_model_names, stack_model_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'configs' from 'c:\\\\Users\\\\user\\\\OneDrive - City University of Hong Kong - Student\\\\CEVR\\\\Cheminformatics\\\\drug-efficacy-prediction-Zaw\\\\util\\\\configs.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH]\n",
      "                             [--n_splits N_SPLITS] [--save_path SAVE_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9013 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"c1d4213c-73be-44ec-8f6d-316ddbec9af6\" --shell=9007 --transport=\"tcp\" --iopub=9014 --f=c:\\Users\\user\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-21788SZ4N7sQmVLvf.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_path = os.path.join(configs.WORK_DIRECTORY,configs.DATA_FILE)\n",
    "    n_splits = 3\n",
    "    save_path = configs.WORK_DIRECTORY\n",
    "\n",
    "    # parse parameters\n",
    "    parser = argparse.ArgumentParser(description='Build base and stacking models')\n",
    "    parser.add_argument('--data_path', help='A path to csv data file')\n",
    "    parser.add_argument('--n_splits', type=int, help='Number of fold for Cross Validation')\n",
    "    parser.add_argument('--save_path', help='A path to save fitted models')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if args.data_path:\n",
    "        data_path = args.data_path\n",
    "    if args.n_splist:\n",
    "        n_splits = args.n_splits\n",
    "    if args.save_path:\n",
    "        save_path = args.save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('p_chem_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73ddc8d69bcdeb99516824a5b6f0f748e02f4ae5c9d52cf5cdf0431a8e350877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
